{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"datalake_test_2\") \\\n",
    "        .getOrCreate()\n",
    "    print('spark session created')\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + 'song_data'\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "    print('song_data loaded')\n",
    "    # extract columns to create songs table\n",
    "    df.createOrReplaceTempView('stagingsongs_table')\n",
    "    print('stagingsongs_table Temp table created')\n",
    "    songs_table= spark.sql('''\n",
    "                        SELECT song_id, \n",
    "                        title, \n",
    "                        artist_id, \n",
    "                        year, \n",
    "                        duration\n",
    "                        FROM stagingsongs_table\n",
    "                        ''')\n",
    "    \n",
    "    print('songs_table created')\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.mode('overwrite').partitionBy('year', 'artist_id').parquet(output_data + 'songs')\n",
    "    print('songs_table file created')\n",
    "    \n",
    "    # extract columns to create artists table\n",
    "    artists_table = spark.sql('''\n",
    "                SELECT artist_id, \n",
    "                artist_name        AS name, \n",
    "                artist_location    AS location, \n",
    "                artist_latitude    AS latitude, \n",
    "                artist_longitude   AS longitude\n",
    "                FROM stagingsongs_table\n",
    "                ''')\n",
    "    print('artist table created')\n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode('overwrite').parquet(output_data + 'artist')\n",
    "    print('artist table file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + 'log_data'\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    print('log_data loaded')\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df[df.page =='NextSong']\n",
    "    print('NextSong data filtered')\n",
    "\n",
    "    # extract columns for users table \n",
    "    df.createOrReplaceTempView('stagingevents_table')\n",
    "    print('stagingevents_table temp table created')\n",
    "    \n",
    "    users_table = spark.sql('''\n",
    "                        SELECT userId    AS user_id, \n",
    "                        firstName        AS first_name, \n",
    "                        lastName         AS last_name, \n",
    "                        gender, \n",
    "                        level\n",
    "                        FROM stagingevents_table\n",
    "                        ''')\n",
    "    print('users table created')\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode('overwrite').parquet(output_data + 'users')\n",
    "    print('users table file created')\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: dt.datetime.fromtimestamp(x / 1000), TimestampType())\n",
    "    df_ts =df.withColumn(\"timestamp\", get_timestamp(df.ts))\n",
    "    print('timestamp column added')\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    #get_datetime = udf()\n",
    "    #df = \n",
    "\n",
    "\n",
    "    # extract columns to create time table\n",
    "    df_ts.createOrReplaceTempView('df_ts_table')\n",
    "    print('df_ts_table temp table created')\n",
    "    \n",
    "    time_table = spark.sql('''\n",
    "                        SELECT timestamp                     AS start_time, \n",
    "                        EXTRACT(hour      FROM timestamp)    AS hour, \n",
    "                        EXTRACT(day       FROM timestamp)    AS day, \n",
    "                        EXTRACT(week      FROM timestamp)    AS week, \n",
    "                        EXTRACT(month     FROM timestamp)    AS month, \n",
    "                        EXTRACT(year      FROM timestamp)    AS year, \n",
    "                        EXTRACT(dayofweek FROM timestamp)    AS weekday\n",
    "                        FROM df_ts_table\n",
    "                        ''')\n",
    "    print('time table created')\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.mode('overwrite').partitionBy('year', 'month').parquet(output_data + 'time')\n",
    "    print('time table file created')\n",
    "    \n",
    "    # read in song data to use for songplays table\n",
    "    song_df = input_data + 'song_data'\n",
    "    s_df = spark.read.json(song_df)\n",
    "    print('song df data read in')\n",
    "    \n",
    "    s_df.createOrReplaceTempView('songs_table')\n",
    "    print('songs_table temp table created')\n",
    "    \n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = spark.sql('''\n",
    "                        SELECT monotonically_increasing_id() AS songplay_id, \n",
    "                        se.timestamp                         AS start_time, \n",
    "                        se.userId                            AS user_id, \n",
    "                        se.level, \n",
    "                        ss.song_id, \n",
    "                        ss.artist_id, \n",
    "                        se.sessionId                         AS session_id, \n",
    "                        se.location, \n",
    "                        se.userAgent                         AS user_agent,\n",
    "                        EXTRACT(month FROM se.timestamp)     AS month,\n",
    "                        EXTRACT(year FROM se.timestamp)      AS year\n",
    "                        FROM df_ts_table as se\n",
    "                        JOIN songs_table as ss\n",
    "                        ON ss.artist_name = se.artist\n",
    "                        ''')\n",
    "    print('songplays table created')\n",
    "    \n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.mode('overwrite').partitionBy('year', 'month').parquet(output_data + 'songplays')\n",
    "    print('songplays table file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session created\n",
      "song_data loaded\n",
      "stagingsongs_table Temp table created\n",
      "songs_table created\n",
      "songs_table file created\n",
      "artist table created\n",
      "artist table file created\n",
      "log_data loaded\n",
      "NextSong data filtered\n",
      "stagingevents_table temp table created\n",
      "users table created\n",
      "users table file created\n",
      "timestamp column added\n",
      "df_ts_table temp table created\n",
      "time table created\n",
      "time table file created\n",
      "song df data read in\n",
      "songs_table temp table created\n",
      "songplays table created\n",
      "songplays table file created\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"data/\"\n",
    "    output_data = \"data/output/\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
